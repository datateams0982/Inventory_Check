{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# tqdm.pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, TimeDistributed, LeakyReLU, Conv1D, BatchNormalization, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "cluster=0\n",
    "with open(f'D:\\\\庫存健診開發\\\\data\\\\Training\\\\processed\\\\Cluster_{cluster}_classification_minmax0_Weekly', 'rb') as fp:\n",
    "    load_list = pickle.load(fp)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = load_list[0], load_list[1], load_list[2], load_list[3], load_list[4], load_list[5]\n",
    "def get_dependent_variable(Y_train, Y_val, Y_test):\n",
    "        training = [item[2] for item in Y_train]\n",
    "        val = [item[2] for item in Y_val]\n",
    "        testing = [item[2] for item in Y_test]\n",
    "\n",
    "        return training, val, testing\n",
    "\n",
    "\n",
    "def categorical_transform(data):\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "    d = np.array(data).reshape(len(data), 1)\n",
    "    Y = onehot_encoder.fit_transform(d)\n",
    "\n",
    "    return Y\n",
    "\n",
    "y_train, y_val, y_test = get_dependent_variable(Y_train, Y_val, Y_test)\n",
    "y_train_encode, y_val_encode, y_test_encode = categorical_transform(y_train), categorical_transform(y_val), categorical_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 197067 samples, validate on 27592 samples\n",
      "Epoch 1/30\n",
      "197067/197067 - 5s - loss: 0.6747 - acc: 0.5838 - val_loss: 0.6717 - val_acc: 0.5903\n",
      "Epoch 2/30\n",
      "197067/197067 - 4s - loss: 0.6666 - acc: 0.5990 - val_loss: 0.6703 - val_acc: 0.5878\n",
      "Epoch 3/30\n",
      "197067/197067 - 4s - loss: 0.6645 - acc: 0.6010 - val_loss: 0.6688 - val_acc: 0.5897\n",
      "Epoch 4/30\n",
      "197067/197067 - 4s - loss: 0.6633 - acc: 0.6027 - val_loss: 0.6724 - val_acc: 0.5827\n",
      "Epoch 5/30\n",
      "197067/197067 - 4s - loss: 0.6627 - acc: 0.6039 - val_loss: 0.6780 - val_acc: 0.5850\n",
      "Epoch 6/30\n",
      "197067/197067 - 4s - loss: 0.6620 - acc: 0.6043 - val_loss: 0.6708 - val_acc: 0.5885\n",
      "Epoch 7/30\n",
      "197067/197067 - 4s - loss: 0.6612 - acc: 0.6058 - val_loss: 0.6715 - val_acc: 0.5921\n",
      "Epoch 8/30\n",
      "197067/197067 - 4s - loss: 0.6610 - acc: 0.6063 - val_loss: 0.6698 - val_acc: 0.5881\n",
      "Epoch 9/30\n",
      "197067/197067 - 4s - loss: 0.6606 - acc: 0.6057 - val_loss: 0.6695 - val_acc: 0.5918\n",
      "Epoch 10/30\n",
      "197067/197067 - 4s - loss: 0.6597 - acc: 0.6078 - val_loss: 0.6683 - val_acc: 0.5896\n",
      "Epoch 11/30\n",
      "197067/197067 - 4s - loss: 0.6593 - acc: 0.6078 - val_loss: 0.6701 - val_acc: 0.5877\n",
      "Epoch 12/30\n",
      "197067/197067 - 4s - loss: 0.6590 - acc: 0.6087 - val_loss: 0.6716 - val_acc: 0.5906\n",
      "Epoch 13/30\n",
      "197067/197067 - 4s - loss: 0.6584 - acc: 0.6083 - val_loss: 0.6696 - val_acc: 0.5910\n",
      "Epoch 14/30\n",
      "197067/197067 - 4s - loss: 0.6578 - acc: 0.6093 - val_loss: 0.6700 - val_acc: 0.5944\n",
      "Epoch 15/30\n",
      "197067/197067 - 4s - loss: 0.6572 - acc: 0.6108 - val_loss: 0.6712 - val_acc: 0.5925\n",
      "Epoch 16/30\n",
      "197067/197067 - 4s - loss: 0.6569 - acc: 0.6111 - val_loss: 0.6733 - val_acc: 0.5893\n",
      "Epoch 17/30\n",
      "197067/197067 - 5s - loss: 0.6564 - acc: 0.6116 - val_loss: 0.6697 - val_acc: 0.5903\n",
      "Epoch 18/30\n",
      "197067/197067 - 5s - loss: 0.6559 - acc: 0.6113 - val_loss: 0.6726 - val_acc: 0.5908\n",
      "Epoch 19/30\n",
      "197067/197067 - 5s - loss: 0.6552 - acc: 0.6124 - val_loss: 0.6707 - val_acc: 0.5899\n",
      "Epoch 20/30\n",
      "197067/197067 - 4s - loss: 0.6548 - acc: 0.6134 - val_loss: 0.6737 - val_acc: 0.5874\n",
      "Epoch 21/30\n",
      "197067/197067 - 4s - loss: 0.6544 - acc: 0.6140 - val_loss: 0.6721 - val_acc: 0.5919\n",
      "Epoch 22/30\n",
      "197067/197067 - 4s - loss: 0.6538 - acc: 0.6148 - val_loss: 0.6728 - val_acc: 0.5892\n",
      "Epoch 23/30\n",
      "197067/197067 - 4s - loss: 0.6533 - acc: 0.6155 - val_loss: 0.6729 - val_acc: 0.5916\n",
      "Epoch 24/30\n",
      "197067/197067 - 5s - loss: 0.6526 - acc: 0.6158 - val_loss: 0.6756 - val_acc: 0.5896\n",
      "Epoch 25/30\n",
      "197067/197067 - 4s - loss: 0.6519 - acc: 0.6168 - val_loss: 0.6734 - val_acc: 0.5879\n",
      "Epoch 26/30\n",
      "197067/197067 - 4s - loss: 0.6514 - acc: 0.6170 - val_loss: 0.6718 - val_acc: 0.5908\n",
      "Epoch 27/30\n",
      "197067/197067 - 4s - loss: 0.6509 - acc: 0.6184 - val_loss: 0.6729 - val_acc: 0.5870\n",
      "Epoch 28/30\n",
      "197067/197067 - 4s - loss: 0.6502 - acc: 0.6186 - val_loss: 0.6723 - val_acc: 0.5879\n",
      "Epoch 29/30\n",
      "197067/197067 - 4s - loss: 0.6496 - acc: 0.6193 - val_loss: 0.6767 - val_acc: 0.5847\n",
      "Epoch 30/30\n",
      "197067/197067 - 4s - loss: 0.6490 - acc: 0.6191 - val_loss: 0.6744 - val_acc: 0.5844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b965fd7240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(np.array(X_train).shape[1], np.array(X_train).shape[2]), padding='causal'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='causal'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu', padding='causal'))\n",
    "model.add(Flatten(name='feature'))\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=optimizers.Nadam(lr=0.001),\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "model.fit(np.array(X_train), y_train_encode, validation_data=(np.array(X_val), y_val_encode), batch_size = 256, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('feature').output)\n",
    "training_feature = intermediate_layer_model.predict(np.array(X_train))\n",
    "val_feature = intermediate_layer_model.predict(np.array(X_val))\n",
    "testing_feature = intermediate_layer_model.predict(np.array(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, \n",
    "                                                        max_depth=10,\n",
    "                                                        max_features='auto',\n",
    "                                                        min_samples_leaf=0.1, \n",
    "                                                        min_samples_split=0.1,\n",
    "                                                        verbose=0, \n",
    "                                                        n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=0.1, min_samples_split=0.1,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(training_feature, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6211760873106369"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(testing_feature, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "Optimization.load_optimal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Conv_layer': 2,\n",
       " 'Dense_layer': 1,\n",
       " 'Dense': [256],\n",
       " 'dropout_ratio': [0.15, 0.25, 0],\n",
       " 'filter': [100, 256],\n",
       " 'kernel': [3, 3],\n",
       " 'padding': ['causal', 'causal'],\n",
       " 'learning_rate': 0.003,\n",
       " 'batch': 256,\n",
       " 'epochs': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Optimization.results['CNN_params'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimization.load_optimal_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc</th>\n",
       "      <th>acc</th>\n",
       "      <th>balance_acc</th>\n",
       "      <th>balance_auc</th>\n",
       "      <th>CNN_params</th>\n",
       "      <th>Model_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.552424</td>\n",
       "      <td>0.553566</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>0.90411</td>\n",
       "      <td>{'Conv_layer': 2, 'Dense_layer': 1, 'Dense': [...</td>\n",
       "      <td>{'classifier': 'RandomForest', 'max_depth': 5,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc       acc balance_acc balance_auc  \\\n",
       "0  0.552424  0.553566    0.849315     0.90411   \n",
       "\n",
       "                                          CNN_params  \\\n",
       "0  {'Conv_layer': 2, 'Dense_layer': 1, 'Dense': [...   \n",
       "\n",
       "                                        Model_params  \n",
       "0  {'classifier': 'RandomForest', 'max_depth': 5,...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Optimization.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
